'''
Does the same model give you the same performance every time?
'''

The NB, Base DT, Top DT, and Perceptron have constant performance over all 10 iterations.

This is because no element of randomness goes into the creation of these models.
If the same training and testing data is used every time, the same priors and conditionals will be generated/calculated during the model creation, and in turn used to calculate the score of classes or the entropy in DTs.

As for the Perceptron, according to the documentation, the initial weights are always set to 1 and  and the learning rate is the same. So, it follows that the final weights will be the same as well.
This is the reason why we do not see any changes in performance.

For the Base MLP and Top MLP, we see changes in performance over the iterations.
This is happening since these models use random weights and bias initialization. So, each iteration will have different initial weights, different bias and this in turn will generate different final weights.
The change in performance is less drastic in the Base MLP because we are not changing the hyper-parameters. While in Top MLP, we are not only using random parameters initialization, we are also experimenting with hyper-parameters.

'''
Comparing performance results
'''

-The NB model has a fairly good performance with the following average metrics:
Average accuracy score: 0.86
Average macro-average F1 score: 0.87
Average weighted-average F1 score: 0.86

-The base DT model preforms better than the NB model. In fact, its average performance is of 100% in all metrics:
Average accuracy score: 1.00
Average macro-average F1 score: 1.00
Average weighted-average F1 score: 1.00

-The TOP-DT generated using the GridSearch used the following hyper-parameters:
{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 3}

It seems like the top DT model found using grid search has very similar hyper-parameters and thus, performs as good as the base DT.

-I was surprised to see the performance of the Base-MLP model.
Its performance was fairly low compared to the other models:
Accuracy score: 0.74
Macro-average F1 score: 0.33
Weighted-average F1 score: 0.66

This could be the result of the 1 single hidden layer of 100 neurones.

-The Top MLP generated by the grid search has very good performance compared to the base MLP:
Accuracy score: 0.98
Macro-average F1 score: 0.99
Weighted-average F1 score: 0.98

The model has the following hyper-parameters:
{'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'solver': 'adam'}

